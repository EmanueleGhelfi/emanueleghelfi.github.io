<!DOCTYPE html>
<html lang="en">
  
  
  <meta name="google-site-verification" content="D1nJAX0_x0Dj8DL8A77WzcxRD_6yCCdSV8FfeNDru64">

  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Learning To Run</title>

  <meta name="author" content="Emanuele Ghelfi" />

  

  <link rel="alternate" type="application/rss+xml" title="Emanuele Ghelfi's Blog - Teaching Machines to Learn" href="/feed.xml" />


  <link rel="apple-touch-icon-precomposed" sizes="57x57" href="/apple-touch-icon-57x57.png" />
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="/apple-touch-icon-114x114.png" />
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="/apple-touch-icon-72x72.png" />
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144x144.png" />
<link rel="apple-touch-icon-precomposed" sizes="60x60" href="/apple-touch-icon-60x60.png" />
<link rel="apple-touch-icon-precomposed" sizes="120x120" href="/apple-touch-icon-120x120.png" />
<link rel="apple-touch-icon-precomposed" sizes="76x76" href="/apple-touch-icon-76x76.png" />
<link rel="apple-touch-icon-precomposed" sizes="152x152" href="/apple-touch-icon-152x152.png" />
<link rel="icon" type="image/png" href="/favicon-196x196.png" sizes="196x196" />
<link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96" />
<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32" />
<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16" />
<link rel="icon" type="image/png" href="/favicon-128.png" sizes="128x128" />
<meta name="application-name" content="&nbsp;"/>
<meta name="msapplication-TileColor" content="#FFFFFF" />
<meta name="msapplication-TileImage" content="mstile-144x144.png" />
<meta name="msapplication-square70x70logo" content="mstile-70x70.png" />
<meta name="msapplication-square150x150logo" content="mstile-150x150.png" />
<meta name="msapplication-wide310x150logo" content="mstile-310x150.png" />
<meta name="msapplication-square310x310logo" content="mstile-310x310.png" />
<!-- everything has to be repeated twice because on 2016-02-01 GitHub pages migrated to jekyll 3; see bug https://github.com/jekyll/jekyll/issues/4439 -->

  
  
    
      <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.0/css/font-awesome.min.css" />
    
  

  
  
    
      <link rel="stylesheet" href="/css/bootstrap.min.css" />
    
      <link rel="stylesheet" href="/css/bootstrap-social.css" />
    
      <link rel="stylesheet" href="/css/main.css" />
    
      <link rel="stylesheet" href="/css/custom.css" />
    
      <link rel="stylesheet" href="/css/particle.css" />
    
  

  
  
    
      <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
    
      <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
    
  

  
  

  
  

  
  

  <!-- Facebook OpenGraph tags -->
  <meta property="og:title" content="Learning To Run" />
  <meta property="og:type" content="website" />

  

  
  <meta property="og:url" content="http://emanueleghelfi.github.io/blog/2019/03/28/learning-to-run.html" />
  

  
  <meta property="og:image" content="" />
  

  <!-- Twitter tags -->
  <meta name="twitter:card" content="summary" />

      <!-- Scripts for ggvis from http://ggvis.rstudio.com/0.1/interactivity.html-->
  <!-- ggvis stuff -->
  <script src="http://ggvis.rstudio.com/0.1/libs/ggvis/lib/jquery-1.11.0.min.js"></script>
  <script src="http://ggvis.rstudio.com/0.1/libs/ggvis/lib/jquery-ui/js/jquery-ui-1.10.4.custom.min.js"></script>
  <script charset="utf-8" src="http://ggvis.rstudio.com/0.1/libs/ggvis/lib/d3.min.js"></script>
  <script src="http://ggvis.rstudio.com/0.1/libs/ggvis/lib/vega.min.js"></script>
  <script src="http://ggvis.rstudio.com/0.1/libs/ggvis/lib/QuadTree.js"></script>
  <script src="http://ggvis.rstudio.com/0.1/libs/ggvis/lib/lodash.min.js"></script>
  <script>var lodash = _.noConflict();</script>
  <script src="http://ggvis.rstudio.com/0.1/libs/ggvis/js/ggvis.js"></script>
  <link rel="stylesheet" type="text/css" href="http://ggvis.rstudio.com/0.1/libs/ggvis/lib/jquery-ui/css/smoothness/jquery-ui-1.10.4.custom.min.css"/>
  <link rel="stylesheet" type="text/css" href="http://ggvis.rstudio.com/0.1/libs/ggvis/css/ggvis.css"/>
  <!-- end of ggvis scripts-->

  <!-- Tags management -->
  
    





  

<!-- Jekyll Ideal Image Slider Include -->
<!-- https://github.com/jekylltools/jekyll-ideal-image-slider-include -->
<!-- v1.8 -->


<!-- seo -->
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Learning To Run | Emanuele Ghelfi’s Blog</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Learning To Run" />
<meta name="author" content="manughelfi" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this article we present our approach for the NIPS 2017 ‘Learning To Run’ challenge." />
<meta property="og:description" content="In this article we present our approach for the NIPS 2017 ‘Learning To Run’ challenge." />
<link rel="canonical" href="http://emanueleghelfi.github.io/blog/2019/03/28/learning-to-run.html" />
<meta property="og:url" content="http://emanueleghelfi.github.io/blog/2019/03/28/learning-to-run.html" />
<meta property="og:site_name" content="Emanuele Ghelfi’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-03-28T08:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Learning To Run" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"manughelfi"},"@type":"BlogPosting","headline":"Learning To Run","dateModified":"2019-03-28T08:00:00+00:00","datePublished":"2019-03-28T08:00:00+00:00","url":"http://emanueleghelfi.github.io/blog/2019/03/28/learning-to-run.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://emanueleghelfi.github.io/blog/2019/03/28/learning-to-run.html"},"description":"In this article we present our approach for the NIPS 2017 ‘Learning To Run’ challenge.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->



</head>


  <body>

    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://emanueleghelfi.github.io">Emanuele Ghelfi's Blog</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
	    
        <li>
		  <a href="/">Home</a>
		</li>
		
        <li>
		  <a href="/portfolio.html">Portfolio</a>
		</li>
		
        <li>
		  <a href="/cv.html">CV</a>
		</li>
		
        <li>
		  <a href="/hike.html">Hike</a>
		</li>
		
        <li>
		  <a href="/aboutme.html">About Me</a>
		</li>
		
      </ul>
    </div>
	
	
	
  </div>
</nav>  

    <!-- TODO this file has become a mess, refactor it -->





<header class="header-section ">

<div class="intro-header no-img">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-heading">
          <h1>Learning To Run</h1>
		  
		  
		  
		  <span class="post-meta">Posted on March 28, 2019</span>
		  
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<script type="text/javascript" async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>


<div class="container">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        
        
        
        

        <div id="header-gh-btns">
          
            
              
                  <iframe src="https://ghbtns.com/github-btn.html?user=MultiBeerBandits&repo=learning-to-run&type=star&count=true" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                
            
              
                  <iframe src="https://ghbtns.com/github-btn.html?user=MultiBeerBandits&repo=learning-to-run&type=watch&v=2&count=true" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                
            
              
                  <iframe src="https://ghbtns.com/github-btn.html?user=MultiBeerBandits&type=follow&count=true" frameborder="0" scrolling="0" width="220px" height="20px"></iframe>
              
            
              
                  <iframe src="https://ghbtns.com/github-btn.html?user=MultiBeerBandits&repo=learning-to-run&type=fork&count=true" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                
            
          
        </div>
      
        
      <!-- tags -->
      <span>[
        
          
          <a href="/tag/machine-learning"><code class="highligher-rouge"><nobr>machine-learning</nobr></code></a>
        
          
          <a href="/tag/reinforcement-learning"><code class="highligher-rouge"><nobr>reinforcement-learning</nobr></code></a>
        
          
          <a href="/tag/competition"><code class="highligher-rouge"><nobr>competition</nobr></code></a>
        
          
          <a href="/tag/nips"><code class="highligher-rouge"><nobr>nips</nobr></code></a>
        
      ]</span>
    
<article role="main" class="blog-post">
    <!-- page content-->
	  <div style="margin:20px;">
<center>
<a href="/files/data/l2run_final.pdf"><button name="button" class="btn btn-primary" href="/files/data/l2run_final.pdf">Paper</button></a> <a href="https://www.slideshare.net/EmanueleGhelfi/learning-to-run-138950609"><button name="button" class="btn btn-primary">Slides</button></a>
</center>
</div>

<p>In this article we present our approach for the NIPS 2017 ”Learning To Run” challenge. The goal of the challenge is to develop a controller able to run in a complex environment, by training a model with Deep Reinforcement Learning methods. We follow the approach of the team Reason8 (3rd place). We begin from the algorithm that performed better on the task, Deep Deterministic Policy Gradient (DDPG). We implement and benchmark several improvements over vanilla DDPG, including parallel sampling, parameter noise, layer normalization and domain specific changes. We were able to reproduce results of the Reason8 team, obtaining a model able to run for more than 30m.</p>

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/HVOrhxypOGg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</center>

<h2 id="background">Background</h2>
<p>Reinforcement Learning (RL) deals with sequential descision making problems. At each time step the agent observes the world state, selects an action and receives a reward.</p>

<center>
<figure>
<a href="/blog/figs/l2run/high_level.png"> <img src="/blog/figs/l2run/high_level.png" style="width: 80%;" alt="Figure 1 - RL" /> </a>
<figcaption> Reinforcement Learning framework </figcaption>
</figure>
</center>

<p>The figure above represents the Reinforcement Learning framework. The agent receives, at each time step a representation \( s \) of the state of the system. The agent takes an action a, according to its policy \( \pi \). The action has an effect on the world. The effect is a transition to the state \( s’ \), according to the world dynamic \( P \). At the same time the agent receives a reward \( r \), based on the action and on the state.
The policy \( \pi \) is often encoded in a neural network.</p>

<center>
<figure>
<a href="/blog/figs/l2run/with_nn.png"> <img src="/blog/figs/l2run/with_nn.png" style="width: 80%;" alt="Figure 2 - RL with NN" /> </a>
<figcaption> Deep Reinforcement Learning </figcaption>
</figure>
</center>

<p>The RL goal is to maximize the expected discounted sum of rewards:</p>

\[J_\pi = \mathbb{E}\left[\sum_{t=0}^H \gamma^t r(s_t,a_t) \right] \, .\]

<p>Where \( J \) represents the objective function, \( \gamma \) is the discount factor, \( r \) is the reward and \( H \) is the horizon length.</p>

<h3 id="how-can-we-achieve-this-goal">How can we achieve this goal?</h3>
<p>Gradient ascent over policy parameters:</p>

\[\theta' = \theta + \eta \nabla_\theta J_\pi .\]

<p>Where the update of the policy parameters \( \theta \)  follows the gradient of the objective function \( \nabla J \). 
A straightforward approach to accomplish this is presented in <a class="citation" href="#Williams1992">(Williams, 1992)</a> with REINFORCE algorithm, that uses Monte Carlo sampling to estimate the performance gradient considering a stochastic policy. Deterministic Policy Gradient (DPG) <a class="citation" href="#dpg">(Silver et al., 2014)</a> expands on this by considering deterministic policies only, for continuous action spaces. To ensure adequate exploration, an off-policy actor-critic algorithm is introduced to learn a deterministic target policy from an exploratory behavior policy. However, directly using neural networks as function approximators leads to unsatisfactory results due to two problems:</p>

<ol>
  <li>most optimization algorithms for neural networks assume that samples are independently and identically distributed, which is not true when samples are generated from exploring sequentially in an environment;</li>
  <li>since the network Q, part of Q-learning algorithms, being updated is also used in calculating the target value, the Q update is prone to divergence.</li>
</ol>

<p>DQN <a class="citation" href="#dqn">(Mnih et al., 2013)</a> implements Q-learning using a deep neural network to approximate the Q function. To address the problem (1.) DQN introduces a replay buffer which stores transitions generated by the environment. During training, a batch of transitions is drawn from the buffer to restore the i.i.d. property. Although, since a maximization over the action space is required, DQN does not scale with high-dimensional and continuous action spaces.
Deep Deterministic Policy Gradient (DDPG) <a class="citation" href="#ddpg">(Lillicrap et al., 2016)</a> solves all the these problems by using:</p>
<ul>
  <li>A deterministic parametrization of the actor \( \pi \), updated in the direction of the gradient of \( Q(s, \pi(s)) \). This is a scalable alternative to global maximization, that is infeasible in  high-dimensional continuous action spaces;</li>
  <li>A replay buffer;</li>
  <li>Separated target networks with soft-updates to improve convergence stability.</li>
</ul>

<h2 id="deep-deterministic-policy-gradient">Deep Deterministic Policy Gradient</h2>

<p>DDPG is a state of the art algorithm in Deep Reinforcement Learning.</p>
<h3 id="off-policy">Off policy</h3>
<p>DDPG is an off-policy method, which means that the optimized policy is different from the behavioral policy. Off-policy algorithm usually allow re-usage of all the samples, whereas on-policy approaches would require to throw them away at each update.</p>
<h3 id="actor-critic">Actor Critic</h3>
<p>Actor critic algorithm uses two networks. The actor network represents the agent policy and outputs an action given a state. The critic network takes as input the pair state action and outputs an estimates of the quality of the action in that state. The two networks used in our project are the followings:</p>

<center>
<figure>
<a href="/blog/figs/l2run/actor_critic.png"> <img src="/blog/figs/l2run/actor_critic.png" style="width: 80%;" alt="Figure 3 - Actor Critic" /></a>
<figcaption>Actor Critic representation </figcaption>
</figure>
</center>

<p>Critic is trained using off-policy data coming from the replay buffer, that is a FIFO queue containing tuples \( (s_t, a_t, r_t, s_{t+1}) \). Critic’s task is to minimize the Bellman error (notice that the policy is deterministic, so we can avoid the expectation over actions):</p>

\[Q(s_t,a_t \mid \theta^Q ) = r(s_t,a_t) + \gamma Q(s_{t+1}, \pi (s_{t+1}  \mid \theta^\pi ) \mid \theta^Q ),\]

<p>where \( \theta^Q \) are parameters of the critic network and \( \theta^\pi \) are actor’s parameters.
It is evident in the equation above that the update step of the weights \( \theta^Q \) comprises \( Q(s_t, a_t \mid \theta^Q) \) in the target, resulting in an iterative update prone to divergence. DDPG solves this problem using <strong>target networks</strong>. Target networks are copies of the actor and critic networks that are only used for computing target values, and softly updated for improving stability:</p>

\[\theta' = \tau \theta + (1-\tau)\theta' \qquad \tau \ll 1,\]

<p>where \( \theta \) are the weights of the original network and \( \theta’ \) the weights of the target networks.</p>

<p>The resulting error for the critic network is:</p>

\[L = \frac{1}{N} \sum_{i=1}^N (y_i - Q(s_i, a_i \mid \theta^Q))^2 \, ,\]

<p>with target:</p>

\[y_i = r_i + \gamma Q(s_i, \pi(s_i \mid \theta'^\pi) \mid \theta'^{Q}).\]

<p>The actor is updated using the estimated deterministic policy gradient, here reported for the sake of completeness:</p>

\[\nabla_{\theta^\pi}J \approx \frac{1}{N} \sum_{i=1}^N \nabla_a Q(s, a \mid \theta^Q) \mid_{s = s_i, a = \pi(s_i)} \nabla_{\theta^\pi} \pi(s \mid \theta^\pi)\mid_{s_i}.\]

<p>Applying the vanilla DDPG algorithm to the learning to run task leads to unsatisfactory results as we can see from the initial video.</p>

<h2 id="ddpg-improvements">DDPG Improvements</h2>

<h3 id="parallel-sampling">Parallel Sampling</h3>
<p>A key step in any reinforcement learning algorithm is the generation of \( (s_t, a_t, r_t, s_{t+1}) \) transitions to gather information from the environment. Since our simulator is very slow, making this step as fast as possible is desirable. Parallelization in our algorithm is implemented through three types of threads: sampling threads, training thread and evaluation threads.
Each sampling thread is tasked with collecting trajectories using the provided policy, pushing them in a common queue and waiting for a new policy.</p>

<center>
<figure>
<a href="/blog/figs/l2run/ddpg.png"> <img src="/blog/figs/l2run/ddpg.png" style="width: 80%;" alt="Figure 4 - DDPG" /> </a>
<a href="/blog/figs/l2run/ddpg_focus_1.png"> <img src="/blog/figs/l2run/ddpg_focus_1.png" style="width: 80%;" alt="Figure 5 - DDPG" /> </a>
<figcaption>Parallel Sampling Illustration </figcaption>
</figure>
</center>

<p>The training thread waits samples from \( m \) sampling threads, stores them in the replay buffer and trains the actor and critic networks for a fixed number of training steps. The new actor network is then sent to the waiting sampling threads that can now restart the sampling process.
Evaluation threads are spawned every fixed number of training steps.
Having multiple sampling threads running policies with different weights improves exploration and results in a substantial performance improvement. We used 20 sampling threads, 1 training thread and 5 evaluation threads in our implementation. We found out in our experiments that \( m=1 \) is a good trade-off between sampling and training.</p>

<h3 id="exploration">Exploration</h3>
<p>To explore we used action noise and parameter noise <a class="citation" href="#param_noise">(Plappert et al., 2017)</a>  in an alternated way. At the beginning of an episode we selected between action noise and parameter noise with 0.7 and 0.3 probability respectively.
Action noise is directly applied to the action selected by the actor network.
We used an Ornstein-Uhlenbeck (OU) <a class="citation" href="#ou_noise">(Uhlenbeck &amp; Ornstein, 1930)</a> process to generate correlated noise for efficient exploration in physics based environments.
Parameter noise perturbs actor network weights to obtain a state dependent exploration, thus more coherent with respect to action noise. The noise used in parameter noise is sampled at the beginning of an episode and it’s kept fixed for all the episode. Parameter noise works well with layer normalization <a class="citation" href="#layer_normalization">(Ba et al., 2016)</a>. 
Layer normalization, as the name says, normalizes the output of a selected layer.
This technique, besides stabilizing training, makes possible to use the same perturbation scale across all network layers. We used layer normalization both for actor and critic networks applying it to all layer except the last one before the non linearity.</p>

<h3 id="states-and-actions-flip">States and actions flip</h3>
<p>The model has a symmetric body, thus it’s easy to increase the sample size by flipping states and actions. Flipping a state means to swap left and right parts of the body, flipping an action means to swap actuations of left and right legs.
States and actions flip is implemented in this way: we sample transitions  \( (s_t, a_t, r_t, s_{t+1}) \) from the replay buffer, flip state components of \( s_t \) and \( s_{t+1} \), flip the action  \( a_t \) and add to the batch original as well as flipped transition. This has an easy interpretation: we know that if action \( a_t \)  in state \( s_t \) , results in state  \( s_{t+1} \)  and in a reward signal \( r_t \), doing the symmetric action with respect to \( a_t\) in the symmetric state with respect to \( s_t \) results in the symmetric state with respect to \( s_{t+1} \) and in the <em>same</em> reward signal \( r_t \). 
Flipping transitions helps in obtaining symmetric policies, that is desirable since running is a symmetric task.</p>

<h2 id="environment">Environment</h2>

<center>
<figure>
<a href="/blog/figs/l2run/our_problem.png"> <img src="/blog/figs/l2run/our_problem.png" style="width: 80%;" alt="Figure 6 - Our Problem" /> </a>
<figcaption> Learning To Run </figcaption>
</figure>
</center>

<p>The agent is a musculoskeletal model including information about muscles, joints and links moving in a 2D environment (no movement is possible along Z axis).
Kinematic quantities are provided for body parts and joints, while activation, fiber length and fiber velocity are provided for muscles.<br />
The total amount of variables describing the state of the agent is 146.
The agent can actuate 9 muscles for each leg, thus \( a \in [0,1]^{18} \). 
In our version of the environment we removed obstacles, obtaining a slightly easier version of the task. 
Reward is defined as the change in the x coordinate of the pelvis for each step plus a small penalty for using ligament forces.
An episode terminates when the agent falls (pelvis \( y &lt; 0.65 \)) or after 1000 steps.
Simulation is based on the OpenSim library that relies on the SimBody physics engine. Due to a complex physics engine the environment is quite slow compared to standard RL environments (Roboschool, Mojoco, etc.) and a step can take seconds, thus it is crucial to use the most sample-efficient method.</p>

<h3 id="environment-modifications">Environment modifications</h3>
<p>We used several modifications of the environment during training to improve efficiency and to help the learning algorithm.</p>

<h4 id="reward">Reward</h4>
<p>We added a small bonus to the reward for each time-step survived. We did not study the contribution of this change thoroughly, but we expect it to add some greediness to the initial training steps to favor policies that keeps the model standing.</p>

<h4 id="environment-accuracy">Environment accuracy</h4>
<p>We used a lower integrator accuracy with respect to the standard one of the simulator obtaining 5x speedup. After some episodes the environment becomes slower with respect to initial episodes, probably for a memory leak. We solved this problem doing a reset of the environment after 100 episodes.</p>

<h4 id="relative-positions">Relative positions</h4>
<p>As mentioned above, position vectors of the model body parts are exposed by the simulator as absolute. For the purpose of learning, keeping an absolute reference frame is undesirable. In fact, being running an approximately periodic task, having the skeleton in some position at a given distance from the origin, should make no difference from having it in the same position at another distance. Therefore, we modified the observation vector by centering the \( x \) coordinates of the body parts around the <strong>pelvis</strong> \( x \). Exploiting such symmetry of the model enables shorter learning time and, most importantly, higher generalization.</p>

<h4 id="state-variables">State variables</h4>
<p>OpenSim exposes a number of variables for a musculoskeletal model. Even though to preserve the Markovian property we should consider them all, many of them are in practice useless for the task to learn. In training our model, two subsets of them were considered and we refer to them as <strong>full-state</strong> and <strong>reduced-state</strong>.
<strong>Reduced-state</strong> comprises the \( x \), \( y\) position of body parts, the rotation and rotational speed of joints, the speed and position of the center of mass, resulting in \( s \in \mathbb{R}^{34}\). Namely body parts are <strong>head, torso, right and left toes and talus</strong> and joints are <strong>ground pelvis and left and right ankles, hips and knees</strong>. 
<strong>Full-state</strong> takes into account all the variables from <strong>reduced-state</strong>, together with the speed and acceleration of body parts and acceleration of joints, resulting in \( s \in \mathbb{R}^{67} \).</p>

<h2 id="results">Results</h2>
<p>For all the experiments we ran DDPG algorithm with the modifications we describe in sections <a href="#ddpg-improvements">DDPG improvements</a> and <a href="#environment-modifications">Environment modifications</a>. We performed an <strong>ablation study</strong> to test the relevance of our main changes to vanilla implementation. We compared the performance of a model trained on the <strong>reduced-state</strong> space with respect to the <strong>full-state</strong> space. Moreover we compared the quality of the models learned with or without state-action-flip and parameter noise, in terms of performance and required training steps.
All the models running on the <strong>reduced-state</strong> configuration share the same architecture for actor and critic networks, with Xavier initialization <a class="citation" href="#pmlr-v9-glorot10a">(Glorot &amp; Bengio, 2010)</a> for the neurons.</p>

<h3 id="state-action-flip-and-parameter-noise">State action flip and parameter noise</h3>
<p>In this experiment we investigated on the importance of <strong>state-action flip</strong> and <strong>parameter noise</strong> (PN) for the learning process. We trained four models for approximately \( 10^6 \) training steps with all the combinations of the two improvements, i.e. with and without state-action flip and parameter noise. From our experimental results, introducing both modifications leads to both better performance, in terms of longer run distance, and a significant speed-up in terms of training steps to reach same distance. 
It is also worth highlighting that the learned model with state-action flip achieved higher performance than the one with PN only. This possibly remarks the importance of domain-specific additions in the context of RL which outperformed an uninformed exploration.</p>

<center>
<figure>
<a href="/blog/figs/l2run/learning_curves.png"><img src="/blog/figs/l2run/learning_curves.png" style="width: 80%;" alt="Figure 7 - Learning Curves" /></a>
<figcaption>Performance and Training Time. On the x axis the training time expressed in steps or hours. On the y axis the performance expressed in meters.</figcaption>
</figure>
</center>

<h3 id="sampling-threads">Sampling threads</h3>
<p>In this experiment we analyzed the impact of the number of sampling threads. We trained two models with 10 and 20 sampling threads respectively. We used the same sampling-training strategy: wait for samples from 1 thread, check the state of the other threads (collecting samples if available), train for 300 steps, send the updated policies to waiting threads that restart the sampling process. The experiment with 20 sampling threads outperformed the experiment with 10  sampling threads. This shows the importance of exploration in this task, as well as the importance of parallelization.</p>

<center>
<figure>
<a href="/blog/figs/l2run/thread_number.png"><img name="fig:thread-number" src="/blog/figs/l2run/thread_number.png" style="width: 80%;" alt="Figure 8 - Thread Number" /> </a>
<figcaption>Effect of the thread number on the performance</figcaption>
</figure>
</center>

<h3 id="full-vs-reduced-state">Full vs Reduced state</h3>
<p>In this experiment we compared the performance of two learned models, the first trained over <strong>full-state</strong> space and the second over <strong>reduced-state</strong> space. The former was trained using a \( [150, 64] \) <strong>elu</strong> actor network and a \( [150, 50]\) <strong>tanh</strong> critic network. The latter was trained with a \( [64, 64] \) <strong>elu</strong> actor network and a \( [64, 32] \) <strong>tanh</strong> critic network. From our experiments, models trained with a <strong>reduce-state</strong> space outperformed those trained with the bigger state space. <strong>Full-state</strong> space introduces several variables that could help in learning a controller for our task, but they also increase the complexity of the model. We did not test thoroughly the networks architecture for the <strong>full-state</strong> space and incrementing the number of neurons might lead to better performance.</p>

<center>
<figure>
<a href="/blog/figs/l2run/reduced_vs_full.png"> <img src="/blog/figs/l2run/reduced_vs_full.png" style="width: 80%;" alt="Figure 9 - Full vs Reduced" /> </a>
<figcaption>Effect of the state representation on the performance </figcaption>
</figure>
</center>

<h2 id="authors">Authors</h2>

<ul>
  <li>Emanuele Ghelfi</li>
  <li>Leonardo Arcari</li>
  <li>Emiliano Gagliardi</li>
</ul>

<h2 id="references">References</h2>

<ol class="bibliography"><li><abbr>[Williams1992]</abbr> <span id="Williams1992">Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. <i>Machine Learning</i>, <i>8</i>(3), 229–256. https://doi.org/10.1007/BF00992696</span></li>
<li><abbr>[dpg]</abbr> <span id="dpg">Silver et al., D. (2014). Deterministic Policy Gradient Algorithms. <i>ICML</i>.</span></li>
<li><abbr>[dqn]</abbr> <span id="dqn">Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning. <i>ArXiv:1312.5602 [Cs]</i>. http://arxiv.org/abs/1312.5602</span></li>
<li><abbr>[ddpg]</abbr> <span id="ddpg">Lillicrap et al., T. P. (2016). <i>Continuous control with deep reinforcement learning</i>. https://arxiv.org/abs/1509.02971</span></li>
<li><abbr>[param_noise]</abbr> <span id="param_noise">Plappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R. Y., Chen, X., Asfour, T., Abbeel, P., &amp; Andrychowicz, M. (2017). Parameter Space Noise for Exploration. <i>CoRR</i>, <i>abs/1706.01905</i>. http://arxiv.org/abs/1706.01905</span></li>
<li><abbr>[ou_noise]</abbr> <span id="ou_noise">Uhlenbeck, G. E., &amp; Ornstein, L. S. (1930). On the Theory of the Brownian Motion. <i>Phys. Rev.</i>, <i>36</i>(5), 823–841. https://doi.org/10.1103/PhysRev.36.823</span></li>
<li><abbr>[layer_normalization]</abbr> <span id="layer_normalization">Ba, J. L., Kiros, J. R., &amp; Hinton, G. E. (2016). Layer Normalization. <i>ArXiv:1607.06450 [Cs, Stat]</i>. http://arxiv.org/abs/1607.06450</span></li>
<li><abbr>[pmlr-v9-glorot10a]</abbr> <span id="pmlr-v9-glorot10a">Glorot, X., &amp; Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In Y. W. Teh &amp; M. Titterington (Eds.), <i>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</i> (Vol. 9, pp. 249–256). PMLR. http://proceedings.mlr.press/v9/glorot10a.html</span></li></ol>

</article>

<div class="row">
  <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">

    
      <!-- Check if any share-links are active -->




<section id = "social-share-section">
  <span class="sr-only">Share: </span>

  
  <!--- Share on Twitter -->
    <a href="https://twitter.com/intent/tweet?text=Learning+To+Run+http://emanueleghelfi.github.io/blog/2019/03/28/learning-to-run.html"
      class="btn btn-social-icon btn-twitter" title="Share on Twitter">
      <span class="fa fa-fw fa-twitter" aria-hidden="true"></span>
      <span class="sr-only">Twitter</span>
    </a>
  

  
  <!--- Share on Facebook -->
    <a href="https://www.facebook.com/sharer/sharer.php?u=http://emanueleghelfi.github.io/blog/2019/03/28/learning-to-run.html"
      class="btn btn-social-icon btn-facebook" title="Share on Facebook">
      <span class="fa fa-fw fa-facebook" aria-hidden="true"></span>
      <span class="sr-only">Facebook</span>
    </a>
  

  
  <!--- Share on LinkedIn -->
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://emanueleghelfi.github.io/blog/2019/03/28/learning-to-run.html"
      class="btn btn-social-icon btn-linkedin" title="Share on LinkedIn">
      <span class="fa fa-fw fa-linkedin" aria-hidden="true"></span>
      <span class="sr-only">LinkedIn</span>
    </a>
  

</section>


    
    <ul class="pager blog-pager">
      
      <li class="previous">
        <a href="/blog/2019/02/12/sacred.html" data-toggle="tooltip" data-placement="top" title="Every Experiment is Sacred">&larr; Previous Post</a>
      </li>
      
      
      <li class="next">
        <a href="/blog/2019/05/06/pyconx-gans.html" data-toggle="tooltip" data-placement="top" title="PyCon X - GANs: From Theory to Production">Next Post &rarr;</a>
      </li>
      
    </ul>
  </div>
</div>


<div class="row disqus-comments">
  <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
    
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">
	    var disqus_shortname = 'emanueleghelfi-github-io';
	    // ensure that pages with query string get the same discussion
            var url_parts = window.location.href.split("?");
            var disqus_url = url_parts[0];	    
	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();
	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>


  </div>
</div>

</div>
</div>
</div>



    <footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          
          <li>
            <a href="https://github.com/EmanueleGhelfi" title="GitHub">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
		  
          <li>
            <a href="https://twitter.com/manughelfi" title="Twitter">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
		  
          <li>
            <a href="mailto:manughelfi1994@gmail.com" title="Email me">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
		  
          <li>
            <a href="https://linkedin.com/in/emanuele-ghelfi-9a408396" title="LinkedIn">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
		  
      
          <li>
            <a href="https://scholar.google.com/citations?user=JJqNoGQAAAAJ&hl=en" title="Scholar">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-graduation-cap fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
		  
		  <li>
			<a href="/feed.xml" title="RSS">
			  <span class="fa-stack fa-lg">
				<i class="fa fa-circle fa-stack-2x"></i>
				<i class="fa fa-rss fa-stack-1x fa-inverse"></i>
			  </span>
			</a>
		  </li>		
          		  
        </ul>
        <p class="copyright text-muted">
		  Emanuele Ghelfi
		  &nbsp;&bull;&nbsp;
		  2024
		  
		  
	    </p>
		<p class="theme-by text-muted">
		  Theme by
		  <a href="https://github.com/daattali/beautiful-jekyll">beautiful-jekyll</a>
		</p>
      </div>
    </div>
  </div>
</footer>

  
    <!-- everything has to be repeated twice because on 2016-02-01 GitHub pages migrated to jekyll 3; see bug https://github.com/jekyll/jekyll/issues/4439 -->











  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script>
      	if (typeof jQuery == 'undefined') {
      	  document.write('<script src="/js/jquery-1.11.2.min.js"></scr' + 'ipt>');
      	}
      </script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
	<script src="/js/bootstrap.min.js"></script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
	<script src="/js/main.js"></script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
	<script src="/js/particles.min.js"></script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
	<script src="/js/particles-config.js"></script>
    
  




	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
		ga('create', 'UA-67791526-3', 'auto');
		ga('send', 'pageview');
	</script>
	<!-- End Google Analytics -->


    <script src="/js/jquery-1.11.2.min.js"></script>
    <style>
    .lightbox {width: 100%; height: 100%; position: fixed; top: 0; left: 0; background: rgba(0,0,0,0.85); z-index: 9999999; line-height: 0; cursor: pointer;}
    .lightbox .img {
        position: relative; 
        top: 50%;
        left: 50%;
        -ms-transform: translateX(-50%) translateY(-50%);
        -webkit-transform: translate(-50%,-50%);
        transform: translate(-50%,-50%);
        max-width: 100%;
        max-height: 100%;
    }
    .lightbox .img img {opacity: 0; pointer-events: none; width: auto;}
    @media screen and (min-width: 1200px) {
        .lightbox .img {
            max-width: 1200px;
        }
    }
    @media screen and (min-height: 1200px) {
        .lightbox img {
            max-height: 1200px;
        }
    }
    .lightbox span {
            display: block; 
            position: fixed; 
            bottom: 13px; 
            height: 1.5em; 
            line-height: 1.4em; 
            width: 100%; 
            text-align: center; 
            color: white;
            text-shadow:
        -1px -1px 0 #000,
        1px -1px 0 #000,
        -1px 1px 0 #000,
        1px 1px 0 #000;  
    }
    
    
    
    
    
    
    .lightbox .videoWrapperContainer {
        position: relative; 
        top: 50%;
        left: 50%;
        -ms-transform: translateX(-50%) translateY(-50%);
        -webkit-transform: translate(-50%,-50%);
        transform: translate(-50%,-50%);
        max-width: 900px;
        max-height: 100%;
    }
    .lightbox .videoWrapperContainer .videoWrapper {
        height: 0;
        line-height: 0;
        margin: 0;
        padding: 0;
        position: relative;
        padding-bottom: 56.333%; /* custom */
        background: black;
    } 
    .lightbox .videoWrapper iframe {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        border: 0;
        display: block;
    }   
    .lightbox #prev, .lightbox #next {height: 50px; line-height: 36px; display: none; margin-top: -25px; position: fixed; top: 50%; padding: 0 15px; cursor: pointer; text-decoration: none; z-index: 99; color: white; font-size: 60px;}
    .lightbox.gallery #prev, .lightbox.gallery #next {display: block;}
    .lightbox #prev {left: 0;}
    .lightbox #next {right: 0;}
    .lightbox #close {height: 50px; width: 50px; position: fixed; cursor: pointer; text-decoration: none; z-index: 99; right: 0; top: 0;}
    .lightbox #close:after, .lightbox #close:before {position: absolute; margin-top: 22px; margin-left: 14px; content: ""; height: 3px; background: white; width: 23px;
    -webkit-transform-origin: 50% 50%;
    -moz-transform-origin: 50% 50%;
    -o-transform-origin: 50% 50%;
    transform-origin: 50% 50%;
    /* Safari */
    -webkit-transform: rotate(-45deg);
    /* Firefox */
    -moz-transform: rotate(-45deg);
    /* IE */
    -ms-transform: rotate(-45deg);
    /* Opera */
    -o-transform: rotate(-45deg);
    }
    .lightbox #close:after {
    /* Safari */
    -webkit-transform: rotate(45deg);
    /* Firefox */
    -moz-transform: rotate(45deg);
    /* IE */
    -ms-transform: rotate(45deg);
    /* Opera */
    -o-transform: rotate(45deg);
    }
    .lightbox, .lightbox * {
        -webkit-user-select: none;  
        -moz-user-select: none;    
        -ms-user-select: none;      
        user-select: none;
    }
    </style>
            
    <script>
    function is_youtubelink(url) {
      var p = /^(?:https?:\/\/)?(?:www\.)?(?:youtu\.be\/|youtube\.com\/(?:embed\/|v\/|watch\?v=|watch\?.+&v=))((\w|-){11})(?:\S+)?$/;
      return (url.match(p)) ? RegExp.$1 : false;
    }
    function is_imagelink(url) {
        var p = /([a-z\-_0-9\/\:\.]*\.(jpg|jpeg|png|gif))/i;
        return (url.match(p)) ? true : false;
    }
    function is_vimeolink(url,el) {
        var id = false;
        $.ajax({
          url: 'https://vimeo.com/api/oembed.json?url='+url,
          async: true,
          success: function(response) {
            if(response.video_id) {
              id = response.video_id;
              $(el).addClass('lightbox-vimeo').attr('data-id',id);
            }
          }
        });
    }
    
    $(document).ready(function() {
        //add classes to links to be able to initiate lightboxes
        $("a").each(function(){
            var url = $(this).attr('href');
            if(url) {
                if(url.indexOf('vimeo') !== -1 && !$(this).hasClass('no-lightbox')) is_vimeolink(url,$(this));
                if(is_youtubelink(url) && !$(this).hasClass('no-lightbox')) $(this).addClass('lightbox-youtube').attr('data-id',is_youtubelink(url));
                if(is_imagelink(url) && !$(this).hasClass('no-lightbox')) {
                    $(this).addClass('lightbox-image');
                    var href = $(this).attr('href');
                    var filename = href.split('/').pop();
                    var split = filename.split(".");
                    var name = split[0];
                    $(this).attr('title',name);
                }
            }
        });
        //remove the clicked lightbox
        $("body").on("click", ".lightbox", function(event){
            if($(this).hasClass('gallery')) {
                
                $(this).remove();
    
                if($(event.target).attr('id')=='next') {
                    //next item
                    if($("a.gallery.current").nextAll("a.gallery:first").length) $("a.gallery.current").nextAll("a.gallery:first").click();
                    else $("a.gallery.current").parent().find("a.gallery").first().click();
                }
                else if ($(event.target).attr('id')=='prev') {
                    //prev item
                    if($("a.gallery.current").prevAll("a.gallery:first").length) $("a.gallery.current").prevAll("a.gallery:first").click();
                    else $("a.gallery.current").parent().find("a.gallery").last().click();
                }
                else {
                    $("a.gallery").removeClass('gallery');
                }
            }
            else $(this).remove();
        });
        //prevent image from being draggable (for swipe)
        $("body").on('dragstart', ".lightbox img", function(event) { event.preventDefault(); });
        //add the youtube lightbox on click
        $("a.lightbox-youtube").click(function(event){
            event.preventDefault();
            $('<div class="lightbox"><a id="close"></a><a id="next">&rsaquo;</a><a id="prev">&lsaquo;</a><div class="videoWrapperContainer"><div class="videoWrapper"><iframe src="https://www.youtube.com/embed/'+$(this).attr('data-id')+'?autoplay=1&showinfo=0&rel=0"></iframe></div></div></div>').appendTo('body');
        });
        //add the image lightbox on click
        $("a.lightbox-image").click(function(event){
            event.preventDefault();
            $('<div class="lightbox"><a id="close"></a><a id="next">&rsaquo;</a><a id="prev">&lsaquo;</a><div class="img" style="background: url(\''+$(this).attr('href')+'\') center center / contain no-repeat;" title="'+$(this).attr('title')+'" ><img src="'+$(this).attr('href')+'" alt="'+$(this).attr('title')+'" /></div><span>'+$(this).attr('title')+'</span></div>').appendTo('body');
        });
        //add the vimeo lightbox on click
        $("body").on("click", "a.lightbox-vimeo", function(event){
            event.preventDefault();
            $('<div class="lightbox"><a id="close"></a><a id="next">&rsaquo;</a><a id="prev">&lsaquo;</a><div class="videoWrapperContainer"><div class="videoWrapper"><iframe src="https://player.vimeo.com/video/'+$(this).attr('data-id')+'/?autoplay=1&byline=0&title=0&portrait=0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div></div></div>').appendTo('body');
        });
    
        $("body").on("click", "a[class*='lightbox-']", function(){
            var link_elements = $(this).parent().find("a[class*='lightbox-']");
            $(link_elements).removeClass('current');
            for (var i=0; i<link_elements.length; i++) {
                if($(this).attr('href') == $(link_elements[i]).attr('href')) {
                    $(link_elements[i]).addClass('current');
                }
            }
            if(link_elements.length>1) {
                $('.lightbox').addClass('gallery');
                $(link_elements).addClass('gallery');
            }
        });
    
        
    });
    
    $(document).keydown(function(e) {
        switch(e.which) {
            case 37: // left
            $("#prev").click();
            break;
            case 39: // right
            $("#next").click();
            break;
        case 27: // esc
            $("#close").click();
            break;
            default: return; // exit this handler for other keys
        }
        e.preventDefault(); // prevent the default action (scroll / move caret)
    });
    
    
    
      /*===========================
      Swipe-it v1.4.1
      An event listener for swiping gestures with vanilla js.
      https://github.com/tri613/swipe-it#readme
     
      @Create 2016/09/22
      @Update 2017/08/11
      @Author Trina Lu
      ===========================*/
    
      "use strict";var _slicedToArray=function(){function n(n,t){var e=[],i=!0,o=!1,r=void 0;try{for(var u,c=n[Symbol.iterator]();!(i=(u=c.next()).done)&&(e.push(u.value),!t||e.length!==t);i=!0);}catch(n){o=!0,r=n}finally{try{!i&&c.return&&c.return()}finally{if(o)throw r}}return e}return function(t,e){if(Array.isArray(t))return t;if(Symbol.iterator in Object(t))return n(t,e);throw new TypeError("Invalid attempt to destructure non-iterable instance")}}();!function(n,t,e){function i(n){function e(){o("touchstart",m,w),o("touchmove",d,w),o("touchend",p,w),E.mouseEvent&&o("mousedown",s,w)}function i(){y=!1,D=!1,A=!1,b=!1,a=!1}function s(n){a=this,y=n.clientX,D=n.clientY,o("mousemove",l,v),o("mouseup",h,v)}function l(n){n.preventDefault(),y&&D&&(A=n.clientX,b=n.clientY)}function h(n){r("mousemove",l,v),r("mouseup",h,v),p(n)}function m(n){a=this,y=n.touches[0].clientX,D=n.touches[0].clientY}function d(n){A=n.touches[0].clientX,b=n.touches[0].clientY}function p(n){if(y&&D&&A&&b){var t=y-A,e=D-b,o=[t,e].map(Math.abs),r=_slicedToArray(o,2),c=r[0],s=r[1],v=E.minDistance;if(c>v){var f=y<A?"swipeRight":"swipeLeft";u(f,a,{distance:t,start:y,end:A})}if(s>v){var l=D>b?"swipeUp":"swipeDown";u(l,a,{distance:e,start:D,end:b})}(c>v||s>v)&&u("swipe",a)}i()}var E=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},w=c(t.querySelectorAll(n)),y=void 0,D=void 0,A=void 0,b=void 0;E.mouseEvent=void 0===E.mouseEvent?f.mouseEvent:E.mouseEvent,E.minDistance=void 0===E.minDistance?f.minDistance:E.minDistance,i(),e(),this.on=function(n,t){return o(n,t,w),this}}function o(n,t,e){s(e).forEach(function(e){return e.addEventListener(n,t)})}function r(n,t,e){s(e).forEach(function(e){return e.removeEventListener(n,t)})}function u(n,e){var i=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{},o=t.createEvent("Event");o.initEvent(n,!0,!0),o.swipe=i,s(e).forEach(function(n){return n.dispatchEvent(o)})}function c(n){for(var t=[],e=0;e<n.length;e++)t.push(n[e]);return t}function s(n){return Array.isArray(n)?n:[n]}var a=!1,v=[n],f={mouseEvent:!0,minDistance:30};n[e]=i}(window,document,"SwipeIt");
    
    var mySwipeIt = new SwipeIt('body');
    mySwipeIt.on('swipeLeft',function(e){
        //check if lightbox is present
        if($('.lightbox').length >  0 ) {
            $("#next").click();
        }
    }).on('swipeRight',function(e){
        //check if lightbox is present
        if($('.lightbox').length >  0 ) {
            $("#prev").click();
        }
    });
    
    </script>
    <!-- Jekyll Ideal Image Slider Include -->
<!-- https://github.com/jekylltools/jekyll-ideal-image-slider-include -->
<!-- v1.8 -->
  
  </body>
</html>